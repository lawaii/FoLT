{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f225a11-3bf2-4f93-8c8b-c944b5c9f24b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a2584fc350017f36496397ea45a3aded",
     "grade": false,
     "grade_id": "cell-89f673ba80d9023b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# <u>Foundations of Language Technology 2023/24</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134f7102-3b09-4f11-801f-5a27e2117df4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "30f1ff11c71522fcb861bc3926bf26cf",
     "grade": false,
     "grade_id": "cell-eb893c1fc94df158",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## <u>Shared Task - SubTask 3 </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0013419-7f29-40ea-b22f-8420eac38c30",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cfd77483b405d3fac4fbef88bf47f46c",
     "grade": false,
     "grade_id": "cell-7630be1a6c50fe26",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### Please enter your group number as well as the name of each group member in the field below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f8147a-356f-4255-812e-c77fdf838bb7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b7bdddf8be7dd66eabe34b8dffa091ec",
     "grade": true,
     "grade_id": "cell-4d0551d9c360aeed",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "Group Number: 5\n",
    "Group Member:\n",
    "\n",
    "Jobst Harzer\r\n",
    "Nehath Nils Mia\r\n",
    "Xiaoyan Xue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cddfe14-d5db-4e23-93a1-7b789e0330ea",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9758612e6e7f94c6271bb7a105b3306b",
     "grade": false,
     "grade_id": "cell-dc28d91f85e0969d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "_**Regarding types, documentation, and output:**_\n",
    "\n",
    "_We have aimed to provide clear descriptions of the tasks and the underlying methods. If anything is unclear, please reach out to us in the Shared Task  discussion forum on Moodle. To enhance clarity, we've included type hints for both function parameters and return values as well as a short description of the method and its components. You are only required to implement the functionality of methods and fill in your code in specified code cells (YOUR CODE HERE)_\n",
    "\n",
    "_While implementing your code, you should use the provided method stubs and parameters. Additionally, make sure that your code runs smoothly without errors and executes within a reasonable amount of time. A recommended practice is to utilize \"Kernel/Restart & Run All\" before submission to verify its functionality._\n",
    "\n",
    "_We encourage the use of comments where necessary to explain your code. Lastly, pay attention to how you output the results._\n",
    "\n",
    "_**Please only modify the template in the specified markdown and code cells(e.g. YOUR CODE / ANSWER) and refrain from modifying other cells. Especially the blank cells are left blank on purpose since they are used to autograde your submission. If you modify these cells the automatic grading will fail for your submission and we might deduct points. The cells containing tests should remain untouched to ensure accurate evaluation. If you wish to conduct additional tests, utilize the provided code cell for your solutions (YOUR CODE HERE). Unfinished methods will contain the following line \"raise NotImplementedError()\", these are used to raise an error if the method is not implemented yet. Please replace this line of code with your actual implementation. You are allowed to write helper functions but please utilize the provided code cell for your solutions (YOUR CODE HERE). No additional imports should be made, you are only allowed to use modules mentioned in the code cells or built-in python functions, which do not need to be imported.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba38900a-7141-45ce-a451-83605d845ace",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b8234a6009c22e76df00ed6042ec127a",
     "grade": false,
     "grade_id": "cell-819e54ad64776370",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Submission:**\n",
    "\n",
    "Please upload your submission to Moodle before  <font color=\"red\">Jan 21th, 23:59pm </font>!\n",
    "\n",
    "Submission format: `Group_XX_Shared_Task_3.zip`( e.g. for Group 29, you should submit the file with the name Group_29_Shared_Task_3.zip). \n",
    "\n",
    "Your submission should contain your filled out Jupyter notebook (naming schema: `Shared_Task 3.ipynb`) and the devset json file `dev.json` which is necessary to run your code.\n",
    "\n",
    "Each submission must be handed in only once per group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef5c0fd-728c-448e-b70c-75a428297390",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "04d2d7b022889eb2666493190afe7bdc",
     "grade": false,
     "grade_id": "cell-174e4312bb0f8433",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1 - 70 Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc45c1c8-5edc-4dc8-b4b8-0e5524c96187",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9162fbf73a556eb56c9efef41622c1fa",
     "grade": false,
     "grade_id": "cell-3e7a3c74f648d3dd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# run this cell to import all modules needed for Task 1\n",
    "from typing import Tuple, List, Dict, DefaultDict, Set\n",
    "import json\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda78134-feb8-4502-9d77-7875b9ff97fa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7b3298f7c6fe072258b3132ee0ce28e",
     "grade": false,
     "grade_id": "cell-aaba2b6036f500ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### __a)__(5 Points) \n",
    "You will be working with the devset which consists of 100 CCAs and 500 QA-Pairs with annotations made by you in the previous Subtasks of the Shared Tasks. Look into the dataset to get a better feel for the data and its structure. For the analysis of the devset and the creation of datasets for training classifiers later, we need to read in the CCAs from the devset and save the relevant information.\n",
    "\n",
    "For this task, go through all QA-Pairs in the file and save them in a list of tuples containing the following information: `question type` refers to the annotated type of the question of the current QA-Pair, `question` refers to the question text of the current QA-Pair,  `gold answer` refers to the given gold answer of the current QA-Pair, `topics` include the topics from topic1 and topic2 of the current QA Pair, `llm model name` refers to the name of the Large Language model that generated the answer of the current QA Pair, `llm answer` refers to the whole answer presented by the specific Large Language model of the current QA-Pair, `llm answer units` refers to the answer units which make up the answer given by the Large Language model of the current QA-Pair, `labels` are the specific labels for each answer unit of the current QA-Pair.\n",
    "Each tuple represents a single QA-Pair and should contain all the above-mentioned information in the order mentioned. All Strings should be processed and read in like in the tutorial for this Shared Task. __Exclude__ QA-Pairs that were annotated as invalid answers (QA-Pairs were considered invalid when the generated answers content were not relevant to the question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f9d06b7-12fb-4215-b3c8-faaf1c7e1740",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7eb71e8daee65e505c109db80794126b",
     "grade": false,
     "grade_id": "cell-f15c7f9da2305ad5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def extract_data(path: str) -> List[tuple]:\n",
    "    \"\"\"\n",
    "    Extracts the relevant information from the devset JSON file containing cca_llm_answers.\n",
    "\n",
    "    Args:\n",
    "        path (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples, each containing information about a QA pair.\n",
    "              Each tuple includes the following elements in this particular order:\n",
    "              - question type: str\n",
    "              - question: str\n",
    "              - gold answer: str\n",
    "              - topics: List[str]\n",
    "              - llm model name: str\n",
    "              - llm answer: str\n",
    "              - llm answer units: List[str]\n",
    "              - labels: List[str]\n",
    "    \"\"\"\n",
    "     \n",
    "    # YOUR CODE HERE\n",
    "    with open(path,\"r\", encoding=\"utf-8\",) as file:\n",
    "        cca_llm_answers = json.load(file)\n",
    "        annotation_entries = []\n",
    "\n",
    "    for qa_pair in cca_llm_answers:\n",
    "        #id = cca_llm_answer[\"id\"]\n",
    "        topics = []\n",
    "        units= []\n",
    "\n",
    "        # Exclude invalid QAs\n",
    "        valid = qa_pair[\"annotations\"][0][\"result\"][0][\"value\"][\"choices\"][0]\n",
    "        if(valid == \"Invalid\"):\n",
    "            continue;\n",
    "\n",
    "        # Split units of the llm_answer\n",
    "        for m in qa_pair[\"annotations\"][0][\"result\"]:\n",
    "            if 'text' in m[\"value\"]:\n",
    "                units.append(m[\"value\"][\"text\"])\n",
    "                # print(m[\"value\"][\"text\"])\n",
    "\n",
    "        # print(qa_pair[\"annotations\"][0][\"result\"][0][\"value\"][\"choices\"][0])\n",
    "            \n",
    "        llm_model = qa_pair[\"data\"][\"llm_model_name\"][:-7]\n",
    "        topic_1 = qa_pair[\"data\"][\"topic1\"][1:-1].replace(\"'\", \"\").split(\", \")\n",
    "        topic_2 = qa_pair[\"data\"][\"topic2\"][1:-1].replace(\"'\", \"\").split(\", \") \n",
    "        question = qa_pair[\"data\"][\"question\"].replace('\\n', '')\n",
    "        gold_answer = qa_pair[\"data\"][\"gold_answer\"].replace('\\n', '')\n",
    "        llm_answer = qa_pair[\"data\"][\"llm_answer\"].replace('\\n', '')\n",
    "        \n",
    "        # Split units of the llm_answer\n",
    "        # units =   qa_pair[\"data\"][\"llm_answer\"].replace('\\n', '.').split(\".\")\n",
    "        # sents = []\n",
    "        # for sent in units:\n",
    "        #     if len(sent) < 15:\n",
    "        #         continue\n",
    "        #     sents.append(sent)\n",
    "            \n",
    "\n",
    "        # Add topics into a list if not empty\n",
    "        for i in topic_1:\n",
    "            if(i != ''):\n",
    "                topics.append(i)\n",
    "        for m in topic_2:\n",
    "            if(m != ''):\n",
    "                topics.append(m)\n",
    "        \n",
    "        # we initalize the values as an empty string or an empty list\n",
    "        question_type, labels = \"\", []\n",
    "\n",
    "        # we iterate through the entries of the values to extract validity, question_type and the labels \n",
    "        for answer_unit in qa_pair[\"annotations\"][0][\"result\"]:\n",
    "                if answer_unit[\"type\"] == \"labels\":\n",
    "                    # Extracting labels and handling the case where labels are not present\n",
    "                    labels.append(answer_unit[\"value\"].get(\"labels\", [\"\"])[0])\n",
    "                elif answer_unit[\"type\"] == \"choices\" and answer_unit[\"origin\"] == \"manual\":\n",
    "                        question_type = answer_unit[\"value\"][\"choices\"][0]\n",
    "\n",
    "        # print(topics)\n",
    "        # print(llm_model)\n",
    "        # print(llm_answer)\n",
    "        # print(\" \")\n",
    "        # if(len(labels)==14):\n",
    "        # print(\"length of units:\")\n",
    "        # print(len(units))\n",
    "        # print(\"length of labels:\")\n",
    "        # print(len(labels))\n",
    "        # print(\" \")\n",
    "\n",
    "        # if(len(units)!=len(labels)):\n",
    "        #     print(\"length of units:\")\n",
    "        #     print(len(units))\n",
    "        #     print(\"length of labels:\")\n",
    "        #     print(len(labels))\n",
    "        \n",
    "        #     for i in sents:\n",
    "        #         print(i)\n",
    "        #         print(\" \")\n",
    "        #     # print(sents)\n",
    "        #     print(labels)\n",
    "        #     print(\" \")\n",
    "\n",
    "        # appending the tuple of information for this qa_pair\n",
    "        annotation_entries.append(\n",
    "            (question_type, question, gold_answer, topics, llm_model, llm_answer, units, labels)\n",
    "        )\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    qa_pairs = annotation_entries\n",
    "        \n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aac88ab-9c5f-4e62-9c8f-66b141fcba14",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "caba2a164bd1df208406fec066ee80b6",
     "grade": true,
     "grade_id": "cell-f2e29f10fff74779",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Public Tests\n",
    "data = extract_data(\"dev.json\")\n",
    "assert len(data[0]) == 8\n",
    "assert (type(data[0])) == tuple\n",
    "assert type(data) == list\n",
    "assert 'Agree with the gold answer' in data[0][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db067ff7-f6af-46c0-bcc7-2283609c5cc7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b0840f8b796bbf1850ad4a3159d52e1",
     "grade": true,
     "grade_id": "cell-0e6c49df6082caeb",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Private Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641db7bd-7788-4d43-b0cb-9ea0e8caf64f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8c858f5f05de9a61a2be50e53589a364",
     "grade": false,
     "grade_id": "cell-9007c16b97b39989",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### __b)__ (15 Points) \n",
    "In this task we want to compare the relationship between the number of LLM answer units and the number of harmful answers with the same amount of answer units (Pay attention to what is considered a harmful answer). Consider for this task all extracted QA-Pairs from the task 1a). Create for this task, a dictionary that maps the number of answer units of a generated answer by a LLM to the number of harmful and non-harmful answers with the same amount of answer units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdb95e16-d99c-4da8-9a4c-39b108da4755",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9441e8b8a4c9e1887f0fce6d0a6e4d30",
     "grade": false,
     "grade_id": "cell-7763c0fc79a00c42",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def map_answer_units_to_harmfulness(qa_pairs: List[Tuple[str, str, str, List[str], str, str, List[str], List[str]]]) -> dict:\n",
    "    \"\"\"\n",
    "    Maps the number of answer units in LLM Answers to the count of harmful and non-harmful answers with the same number of answer units.\n",
    "\n",
    "    Args:\n",
    "        qa_pairs (List[tuple]): List of tuples containing information about QA pairs.\n",
    "\n",
    "    Returns:\n",
    "        dict[int, dict[str, int]]: A dictionary where keys represent the number of answer units per LLM answer, \n",
    "                                   and values are dictionaries with counts of harmful and non-harmful answers.\n",
    "                                   The nested dictionaries have keys 'harmful_answers' and 'non_harmful_answers'.\n",
    "    \"\"\"\n",
    "    answer_units_dict = {}\n",
    "    # YOUR CODE HERE\n",
    "    def create_llm_models_dict(data):\n",
    "        '''\n",
    "        Creates a dictionary mapping llm_model names to a list of associated labels.\n",
    "    \n",
    "        Args:\n",
    "            data(list of tuples): the extracted data from the function `extract_data`.\n",
    "    \n",
    "        Returns:\n",
    "            dict[str, list[str]]: A dictionary where keys are llm_model names and values are lists of associated labels.\n",
    "        '''\n",
    "        # defaultdict handles the case where the key does not exist in the dictionary yet by creating a new key,value pair\n",
    "        llm_model_labels = defaultdict(list)\n",
    "        \n",
    "        for qa_pair in data:\n",
    "            units, label = qa_pair[6], qa_pair[7]\n",
    "            \n",
    "            # print(label)\n",
    "\n",
    "            harm = 0\n",
    "            non_harm = 0\n",
    "\n",
    "            for i in label:\n",
    "\n",
    "                if i == \"Contradiction\" or i == \"Exaggeration\":\n",
    "                    harm = harm + 1\n",
    "                else:\n",
    "                    non_harm  = non_harm + 1\n",
    "                    \n",
    "            # Extend the list of labels associated with the current llm_model\n",
    "            if harm > non_harm:\n",
    "                llm_model_labels[len(units)].extend([\"harm\"])\n",
    "            else:\n",
    "                llm_model_labels[len(units)].extend([\"non_harm\"])\n",
    "\n",
    "            \n",
    "            \n",
    "                \n",
    "            #     if i == \"Contradiction\" or i == \"Exaggeration\":\n",
    "            #         harm = harm + 1\n",
    "            #     else:\n",
    "            #         non_harm  = non_harm + 1\n",
    "                    \n",
    "            # # Extend the list of labels associated with the current llm_model\n",
    "            # if harm > non_harm:\n",
    "            #     llm_model_labels[len(units)].extend(\"harmful\")\n",
    "            # else:\n",
    "            #     llm_model_labels[len(units)].extend(\"non_harmful\")\n",
    "                \n",
    "                \n",
    "            \n",
    "            # Extend the list of labels associated with the current llm_model\n",
    "            # llm_model_labels[len(units)].extend(label)\n",
    "    \n",
    "        # Convert the defaultdict to a regular dictionary before returning\n",
    "        return dict(llm_model_labels)\n",
    "\n",
    "    def count_label(category_labels_dict):\n",
    "        '''\n",
    "        Count the occurrences of each label for each category in a given label dictionary.\n",
    "    \n",
    "        Parameters:\n",
    "        - label_dict (dict[str, List[str]]): Dictionary mapping categories to lists of labels.\n",
    "    \n",
    "        Returns:\n",
    "        - dict[str, dict[str, int]]: Dictionary containing label names and their associated counts for each category.\n",
    "        '''\n",
    "        # Creating a dict by counting occurrences of labels for each category using Counter \n",
    "        return {category: dict(Counter(labels)) for category, labels in category_labels_dict.items()}\n",
    "\n",
    "    def count_harmful_labels(label_occurences):\n",
    "        '''\n",
    "        Calculate the count of harmful and non-harmful labels for each category.\n",
    "    \n",
    "        Args:\n",
    "            label_occurences (dict[str, dict[str, int]]): A dictionary mapping categories to label names and their associated counts.\n",
    "    \n",
    "        Returns:\n",
    "            (dict[str, dict[str, int]]): A dictionary containing counts of harmful and non-harmful labels for each category.\n",
    "        '''\n",
    "        \n",
    "        harmful_label_count = {}\n",
    "        for category, labels_dict in label_occurences.items():\n",
    "            \n",
    "            # Calculate the occurences of harmful labels ('Exaggeration' and 'Contradiction') for the current category.\n",
    "            harmful_count = labels_dict.get('harm', 0)\n",
    "            # Calculate the occurences of non-harmful labels for the current category by excluding Exaggeration, Contradiction.\n",
    "            non_harmful_count = sum(value for key, value in labels_dict.items() if key not in {'harm'})\n",
    "            #  Create a dictionary entry for the current category, containing counts of harmful and non-harmful labels.\n",
    "            harmful_label_count[category] = {'harmful_answers': harmful_count, 'non_harmful_answers': non_harmful_count}\n",
    "       \n",
    "        return harmful_label_count\n",
    "\n",
    "    llm_models_dict = create_llm_models_dict(data)\n",
    "\n",
    "    # print(llm_models_dict)\n",
    "    \n",
    "    label_occurences = count_label(llm_models_dict)\n",
    "\n",
    "    # print(label_occurences)\n",
    "    # print(\" \")\n",
    "    \n",
    "    harmful_label_occurences = count_harmful_labels(label_occurences)\n",
    "\n",
    "    # print(harmful_label_occurences)\n",
    "\n",
    "    answer_units_dict = harmful_label_occurences\n",
    "\n",
    "    print(answer_units_dict)\n",
    "    print(\" \")\n",
    "\n",
    "    print(\"answer_units_dict[14]\")\n",
    "    print(answer_units_dict[14])\n",
    "    print(\"sum(answer_units_dict[14].values())\")\n",
    "    print(sum(answer_units_dict[14].values()))\n",
    "\n",
    "    # print(answer_units_dict[1])\n",
    "\n",
    "    return answer_units_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c981634-7309-48d9-ba16-8ea0751cca75",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a134e2acd1ae9faabb39c9ca45d1efc",
     "grade": true,
     "grade_id": "cell-0ca651ec01cb1c28",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {'harmful_answers': 1, 'non_harmful_answers': 3}, 5: {'harmful_answers': 5, 'non_harmful_answers': 69}, 3: {'harmful_answers': 14, 'non_harmful_answers': 45}, 9: {'harmful_answers': 2, 'non_harmful_answers': 26}, 13: {'harmful_answers': 0, 'non_harmful_answers': 11}, 8: {'harmful_answers': 1, 'non_harmful_answers': 32}, 11: {'harmful_answers': 0, 'non_harmful_answers': 20}, 2: {'harmful_answers': 8, 'non_harmful_answers': 25}, 7: {'harmful_answers': 4, 'non_harmful_answers': 28}, 4: {'harmful_answers': 4, 'non_harmful_answers': 87}, 14: {'harmful_answers': 0, 'non_harmful_answers': 10}, 12: {'harmful_answers': 1, 'non_harmful_answers': 12}, 10: {'harmful_answers': 1, 'non_harmful_answers': 22}, 6: {'harmful_answers': 2, 'non_harmful_answers': 40}, 15: {'harmful_answers': 0, 'non_harmful_answers': 7}, 17: {'harmful_answers': 0, 'non_harmful_answers': 2}, 18: {'harmful_answers': 0, 'non_harmful_answers': 4}, 22: {'harmful_answers': 0, 'non_harmful_answers': 1}, 16: {'harmful_answers': 0, 'non_harmful_answers': 1}, 19: {'harmful_answers': 0, 'non_harmful_answers': 1}}\n",
      " \n",
      "answer_units_dict[14]\n",
      "{'harmful_answers': 0, 'non_harmful_answers': 10}\n",
      "sum(answer_units_dict[14].values())\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Public Tests\n",
    "answer_units_dict = map_answer_units_to_harmfulness(data)\n",
    "assert isinstance(answer_units_dict, dict)\n",
    "assert len(answer_units_dict[1].values()) == 2\n",
    "assert 5 in answer_units_dict.keys()\n",
    "assert sum(answer_units_dict[14].values()) == 10\n",
    "assert answer_units_dict[1] == {'harmful_answers': 1, 'non_harmful_answers': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "294fa369-9404-48bb-9691-9f04e18ed560",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2912332ad6c13fda6de5756afb612aee",
     "grade": true,
     "grade_id": "cell-ea13af779385c92d",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Private Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c228ceff-6160-49f2-b872-ae287e9bb50d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "06b2635b5d98447c185f1726e9838223",
     "grade": false,
     "grade_id": "cell-67e15e0a27e52503",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### __c)__ (15 Points) \n",
    "Save the 10 answer units with the highest relative amount of harmful answers to the total number of answers with this amount of answer units. Only consider answer units with at least 10 total answers(including harmful and non-harmful answers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41c8912c-cad6-421b-9385-5ffe406a662d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0a6da7b260eaa8187c0ae0b93f78cfa",
     "grade": false,
     "grade_id": "cell-3aaa9a9f7e2d25af",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_top_answer_units(answer_units_dict: Dict[int, Dict[str, int]]) -> List[Tuple[int, float]]:\n",
    "    \"\"\"\n",
    "    Find the top 10 answer units sorted by the fraction of harmful answers among the total answers for each number of answer units.\n",
    "\n",
    "    Args:\n",
    "        answer_units_dict (Dict[int, Dict[str, int]]): Dictionary mapping the number of answer units to counts of harmful and non-harmful answers.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[int, float]]: A list of tuples where each tuple contains the number of answer units and the fraction of harmful answers among the total answers.\n",
    "    \"\"\"\n",
    "    top_answer_units = []\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # A function that returns the length of the value:\n",
    "    def min_func(e):\n",
    "      return e[1]\n",
    "    \n",
    "    min_fraction = 1.0\n",
    "\n",
    "    for i in answer_units_dict:\n",
    "        # Check if the category exists in the dictionary\n",
    "        labels, counts = zip(*answer_units_dict[i].items())\n",
    "        total_amount_labels = sum(counts)\n",
    "        total_dist = 0\n",
    "        if total_amount_labels < 10:\n",
    "            continue\n",
    "        # print(\"\\nThe distributions for the number of units \"+ str(i) + \" are as following: \\n\") \n",
    "\n",
    "        units = (i,counts[0]/total_amount_labels)\n",
    "\n",
    "        if len(top_answer_units) < 10:\n",
    "            top_answer_units.append(units)\n",
    "            min_fraction = min(min_fraction, counts[0]/total_amount_labels) \n",
    "        elif min_fraction != max(min_fraction, counts[0]/total_amount_labels):\n",
    "            top_answer_units.append(units)\n",
    "\n",
    "            top_answer_units.sort(key = min_func)\n",
    "            # print(\"min\")\n",
    "            # print(top_answer_units[0])\n",
    "            top_answer_units.remove(top_answer_units[0])\n",
    "                    \n",
    "            min_fraction = top_answer_units[0][1]\n",
    "            \n",
    "            \n",
    "        # print(units)\n",
    "        # print(min_fraction)\n",
    "\n",
    "        # Show distributions if specified\n",
    "        # for i in range(len(labels)):\n",
    "        #     print(\"The distribution for \"+ labels[i] + \" is \" +str(counts[i]/total_amount_labels))\n",
    "\n",
    "    print(top_answer_units)\n",
    "    \n",
    "    return top_answer_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1779057-c4da-4b87-b1da-f40e66d435e8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77e92e4f29514990a154d881bacf8031",
     "grade": true,
     "grade_id": "cell-98a89f0236a61e1d",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, 0.030303030303030304), (10, 0.043478260869565216), (4, 0.04395604395604396), (6, 0.047619047619047616), (5, 0.06756756756756757), (9, 0.07142857142857142), (12, 0.07692307692307693), (7, 0.125), (3, 0.23728813559322035), (2, 0.24242424242424243)]\n"
     ]
    }
   ],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Public Tests\n",
    "top_answer_units = find_top_answer_units(answer_units_dict)\n",
    "assert isinstance(top_answer_units, list)\n",
    "assert len(top_answer_units) == 10\n",
    "assert all(isinstance(element, tuple) for element in top_answer_units)\n",
    "assert all(isinstance(element[0], int) for element in top_answer_units)\n",
    "assert all(isinstance(element[1], float) for element in top_answer_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e4dda96-26cc-4de6-a6b9-0e4c42b0761c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "39a6ec573033e118031fde4b307cf92b",
     "grade": true,
     "grade_id": "cell-6a713967fa2400b3",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Private Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66c65b8-0c05-4528-ade2-7c4b7a97e3fd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0fcc0fa35f915ea562cd1901d5d8e324",
     "grade": false,
     "grade_id": "cell-ef755c8dc01fd9b0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### __d)__ (15 Points)\n",
    "Now we want to examine how often each label(6 label categories) occurs for every combination of large language model, question type and a single topic among all QA-Pairs. In this task, you need to create a dictionary that maps each combination of LLM model, question type, and a single topic to a dictionary that has entries for each label category and its occurrences. Only include combinations in which all llm models, question type and the single topic are all non-empty and include characters.  **Hint:** You may use defaultdict and Counter from Collections for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5dfc9fd5-f3b7-4f0a-81d7-a8d7d20e7dca",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0d21bbdd34017efd1d303f67bbd0c56",
     "grade": false,
     "grade_id": "cell-e14ed5f6aa177e92",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_combination_label_mapping(qa_pairs: List[Tuple[str, str, str, List[str], str, str, List[str], List[str]]]) -> dict:\n",
    "    \"\"\"\n",
    "    Generates a dictionary mapping every combination of question type, topic, and llm model name\n",
    "    to a dictionary of labels and their total occurrences per category.\n",
    "\n",
    "    Args:\n",
    "        qa_pairs (List[tuple]): List of tuples containing information about QA pairs.\n",
    "\n",
    "    Returns:\n",
    "        dict[[tuple], dict[str: int]]: A dictionary mapping every combination of question type, topic, and llm model name\n",
    "              to a dictionary of labels and their total occurrences.\n",
    "    \"\"\"\n",
    "    combination_label_mapping = {}\n",
    "    # YOUR CODE HERE\n",
    "    c = Counter({'Exaggeration': 0,'Contradiction': 0,'Cannot assess': 0, 'Agree with the gold answer': 0, 'General comment': 0,  'Understatement': 0})\n",
    "          \n",
    "    \n",
    "    for i in qa_pairs:\n",
    "        for m in i[3]:\n",
    "            combination = (i[0],m,i[4])\n",
    "            # print(combination)\n",
    "        # print(type(combination_label_mapping))\n",
    "            counter = Counter(i[7])\n",
    "        # print(counter)\n",
    "\n",
    "        # if combination == ('2. Open ended - Comparison of different specific interventions', 'neurology', 'ChatGPT_prompt0'):\n",
    "        #     print(Counter(i[7]))\n",
    "        # if(i[0] == '2. Open ended - Comparison of different specific interventions' and i[4] == 'ChatGPT_prompt0' and 'neurology' in i[3]):\n",
    "        #     print(\"Question Type:\")\n",
    "        #     print(i[0])\n",
    "        #     print(\"Question:\")\n",
    "        #     print(i[1])\n",
    "        #     print(\"Tpoics:\")\n",
    "        #     print(i[3])\n",
    "        #     print(\"Model Name:\")\n",
    "        #     print(i[4])\n",
    "        #     print(\"Labels:\")\n",
    "        #     print(i[7])\n",
    "        #     print(\"\")\n",
    "\n",
    "        \n",
    "            if combination not in combination_label_mapping:\n",
    "                combination_label_mapping[combination] = Counter(i[7])\n",
    "            else:\n",
    "            # print(\"Before\")\n",
    "            # print(combination)\n",
    "            # print(combination_label_mapping[combination])\n",
    "            # print(\"\")\n",
    "            # print(\"Add\")\n",
    "            # print(Counter(i[7]))\n",
    "            # print(\"\")\n",
    "                combination_label_mapping[combination] = combination_label_mapping[combination] + Counter(i[7])\n",
    "            # print(\"After\")\n",
    "            # print(combination)\n",
    "            # print(combination_label_mapping[combination])\n",
    "            # print(\"\")\n",
    "            combination_label_mapping[combination].subtract(c)\n",
    "\n",
    "        \n",
    "    # print(combination_label_mapping)\n",
    "    # for value in combination_label_mapping.values():\n",
    "    #     if len(value) != 6:\n",
    "    #         print(value)\n",
    "    print(\"Result:\")\n",
    "    print(label_combinations[('2. Open ended - Comparison of different specific interventions', 'neurology', 'ChatGPT_prompt0')])\n",
    "    print(\"Expected:\")\n",
    "    print({'Exaggeration': 0, 'Contradiction': 0, 'Cannot assess': 3, 'Agree with the gold answer': 5, 'General comment': 1, 'Understatement': 0})\n",
    "    return combination_label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cc2407b4-c50d-4ee1-a095-6841e054e144",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "22e6d571b028fbe896d8538708ceebc0",
     "grade": true,
     "grade_id": "cell-49b5a92dd1f8af89",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:\n",
      "Counter({'Agree with the gold answer': 7, 'Cannot assess': 4, 'General comment': 3, 'Exaggeration': 2, 'Contradiction': 1, 'Understatement': 0})\n",
      "Expected:\n",
      "{'Exaggeration': 0, 'Contradiction': 0, 'Cannot assess': 3, 'Agree with the gold answer': 5, 'General comment': 1, 'Understatement': 0}\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mdict\u001b[39m)  \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m label_combinations\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(value) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m label_combinations\u001b[38;5;241m.\u001b[39mvalues()) \n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m label_combinations[(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2. Open ended - Comparison of different specific interventions\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneurology\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChatGPT_prompt0\u001b[39m\u001b[38;5;124m'\u001b[39m)] \u001b[38;5;241m==\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExaggeration\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContradiction\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot assess\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAgree with the gold answer\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGeneral comment\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnderstatement\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m}\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Public Tests\n",
    "label_combinations = create_combination_label_mapping(data)\n",
    "assert isinstance(label_combinations, dict)\n",
    "assert all(isinstance(value, dict)  for value in label_combinations.values())\n",
    "assert all(len(value) == 6 for value in label_combinations.values()) \n",
    "assert label_combinations[('2. Open ended - Comparison of different specific interventions', 'neurology', 'ChatGPT_prompt0')] == {'Exaggeration': 0, 'Contradiction': 0, 'Cannot assess': 3, 'Agree with the gold answer': 5, 'General comment': 1, 'Understatement': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "70394ff7-6b6c-4c7f-930b-a29cce6a6423",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc9e85d33a95468e3218b742412192c3",
     "grade": true,
     "grade_id": "cell-f49486f641eeb11c",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Private Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4ce4ef-2cd8-422d-8949-122b8052a635",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jp-MarkdownHeadingCollapsed": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "35ab83caa0fc50ddae0aa1d02334a29c",
     "grade": false,
     "grade_id": "cell-f8e36658c2b6701e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### __e)__ (20 Points) \n",
    "In this task, we want to find the combinations with the highest occurrence for each label category(6). Create a dictionary that maps each label category to a list of combinations like in 1d) and its occurences for this label category but only include entries that have the highest occurence for a label category . If multiple combinations appear the same amount of times for this label category, all of them should be represented in the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daccbb5d-f5c5-4e46-8401-4df9e52f41bf",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92df8fe4afd08fac84074f6301ab8581",
     "grade": false,
     "grade_id": "cell-3b43b2cd11305487",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_highest_combinations(combination_label_mapping: dict[Tuple[str, str, str], dict[str, int]]) -> dict:\n",
    "    \"\"\"\n",
    "    Finds combinations with the highest occurrence for each label.\n",
    "\n",
    "    Args:\n",
    "        combination_label_mapping (Dict): A dictionary mapping combinations to labels and their occurrences.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, List[Tuple[Tuple[str, str, str], int]]]: A dictionary mapping each label to a list of combinations with the highest occurrence of this label.\n",
    "                                                           Each entry in the list is a tuple containing the combination and its occurrence.\n",
    "    \"\"\"\n",
    "    result_mapping = {}\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return result_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a8a125-e34d-4afc-9977-63b4379d21f8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "73a581a0e1be5a8b03d3a7800226664f",
     "grade": true,
     "grade_id": "cell-aeda322581276626",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Public Tests\n",
    "highest_combinations = find_highest_combinations(label_combinations)\n",
    "assert isinstance(highest_combinations, dict)\n",
    "assert all(isinstance(value, list)  for value in highest_combinations.values())\n",
    "assert all(len(value[0]) == 2 for value in highest_combinations.values()) \n",
    "assert all(len(value[0][0]) == 3 for value in highest_combinations.values()) \n",
    "assert all(isinstance(value[0][1], int) for value in highest_combinations.values())\n",
    "assert len(highest_combinations.items()) == 6\n",
    "assert highest_combinations['General comment'] == [(('4. Open ended - General effects of a specific intervention', 'dementia', 'bingchat_prompt0'), 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12724e7f-27be-426b-a905-32516909b6d0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc253e559e11208b5291bd9845d7f5f5",
     "grade": true,
     "grade_id": "cell-c5ca3092586b64ae",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Private Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3f7c3c-2c0c-470a-92d8-5d9f93404e5f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jp-MarkdownHeadingCollapsed": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a2c359d6d8f99c2dab919d79ff4e7a1a",
     "grade": false,
     "grade_id": "cell-226d5da87b0db685",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 2 - 30 Points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7088436-b701-45f5-bb49-7bd3772b470b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea45b239493efa081f9a451344252e3d",
     "grade": false,
     "grade_id": "cell-c48f07f59735f624",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In task 2 we will build a Decision Tree classifier and a Neural Network classifier and evaluate both afterwards. The first classifying task will ask the classifier to predict whether a given LLM answer is harmful or not. In the second classifying task, the classifier has to predict the label (6 categories) of a specific LLM answer unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ba0b3d-beae-47dc-b37b-40336fb29e71",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf50ac96ae90d9e81e3d38567e797a8f",
     "grade": false,
     "grade_id": "cell-5fc2d855cf651a7f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# run this cell to import all modules needed for Task 2\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "## download this modules in case you have not downloaded them yet\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46f63a1-2a7b-48be-9ee1-d46883823bbe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40f23872c769e78c69776e44b667d9f5",
     "grade": false,
     "grade_id": "cell-b24bc35d7fd2af88",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### __a)__ (15 Points)\n",
    "To achieve the first classification task mentioned above, we need to create a training and test split for the classifier. The input for the classifier should contain for each answer the question, the gold answer, the generated answer by the LLM, and additional features chosen by you that will improve the model's performance. You may use the extracted QA-Pairs from 1a). The dataset should be shuffled with the random seed 42. After shuffling the data, the dataset should be split on question level, meaning 80% of the QA-Pairs should be in the training split and 20% of the QA-Pairs should be in the test split. Prepare the data in such a way that for each data point all the following features: `question`,  `gold answer`, `large language model name`, `llm answer`, `num_answer_units`(the number of answer units generated by the LLM for each answer), `div_of_tokens`(the ratio of unique tokens of the LLM answer compared to all tokens of the generated answer by the LLM), `avg_len_words`(the average number of characters of all words for the given LLM answer) are represented as well as one of the two label categories(`harmful` and `non_harmful`). All of the highlighted features should be included and the features `num_answer_units`,`div_of_tokens` and `avg_len_words` should be only used for this task. Use **word_tokenize()** from nltk to tokenize the text, lowercase the tokens and remove punctuation and stopwords using `stopwords` from `nltk.corpus`. This task will be tested by testing functions and by evaluating the classifier's performance on the created datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1440ecae-22de-41c2-8fdd-b84642d09de2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "effb7ce9785d8ffeb0ef37ae83ef2a46",
     "grade": false,
     "grade_id": "cell-9eed30e31447290e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_train_test_split_answer(qa_pairs: List[Tuple[str, str, str, List[str], str, str, List[str], List[str]]]) -> Tuple[List[Tuple[dict, str]], List[Tuple[dict, str]]]:\n",
    "    '''\n",
    "    Create a feature set and perform a train-test split. The train split is used to train the classifiers on. The test split is used to evaluate the classifiers on.\n",
    "\n",
    "    Args:\n",
    "        qa_pairs (List[tuple]): List of tuples containing information about QA pairs.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[Tuple[dict, str]], List[Tuple[dict, str]]]: \n",
    "        A tuple containing two lists:\n",
    "        - The first list is the train split, where each entry is a tuple with a feature dictionary and its label (\"harmful\" or \"non_harmful\").\n",
    "        - The second list is the test split, with the same format.\n",
    "        - the feature dictionary should look like this: {\"question\": _, \"gold_answer\": _, \"llm_answer\": _, \"llm_model_name\": _,\"num_answer_units\": _,\"div_of_tokens\": _, \"avg_len_words\": _}\n",
    "        Example: The features: num_answer_units, div_of_tokens and avg_len_words would be: {num_answer_units': 4, 'div_of_tokens': 0.7538461538461538, 'avg_len_words': 5.859813084112149} for the following LLM answer: \n",
    "                 \"Macrolides have been studied as a potential treatment for chronic asthma, and their effectiveness compared to placebo varies depending on the specific study and patient population.\n",
    "                 Some studies suggest that macrolides may help reduce asthma exacerbations and improve lung function in certain individuals with chronic asthma, while others show no significant difference compared to a placebo. \n",
    "                 The effectiveness of macrolides in asthma management may depend on factors such as the patient's asthma phenotype, the specific macrolide used, and the duration of treatment. \n",
    "                 Therefore, it's essential to consult with a healthcare professional to determine if macrolides are a suitable option for managing chronic asthma in a particular case.\" \n",
    "    '''\n",
    "    train_split, test_split = [],[]\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return train_split, test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb867e0d-bbe9-47b7-be62-8e9e0687c651",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "029f4c49eabdab426deef84c0ab26ecd",
     "grade": true,
     "grade_id": "cell-b7817e2b3debbda9",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Public Tests\n",
    "from numpy import isclose\n",
    "train_split_answer, test_split_answer = create_train_test_split_answer(data)\n",
    "assert isinstance(train_split_answer, list)\n",
    "assert isinstance(test_split_answer, list)\n",
    "assert all(isinstance(element[0], dict) for element in train_split_answer) \n",
    "assert all(isinstance(element[1], str) for element in train_split_answer) \n",
    "assert all(isinstance(element[0], dict) for element in test_split_answer) \n",
    "assert all(isinstance(element[1], str) for element in test_split_answer) \n",
    "assert all(\"num_answer_units\" in element[0].keys() for element in train_split_answer)\n",
    "assert all(\"num_answer_units\" in element[0].keys() for element in test_split_answer)\n",
    "assert all(\"div_of_tokens\" in element[0].keys() for element in train_split_answer)\n",
    "assert all(\"div_of_tokens\" in element[0].keys() for element in test_split_answer)\n",
    "assert all(\"avg_len_words\" in element[0].keys() for element in train_split_answer)\n",
    "assert all(\"avg_len_words\" in element[0].keys() for element in test_split_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ac79d2-7aa8-44be-a502-0cc46d65ef27",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ce952bd0b7abf96dbaff65eb73ba67b",
     "grade": true,
     "grade_id": "cell-c96b96f1cf7c2259",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Private Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9698dcb2-e6a8-45c3-85de-52806a52c454",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a9aba78ec460de6144853c7abcc00638",
     "grade": false,
     "grade_id": "cell-4a7b9fdf456208c1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### __b)__ (15 Points)\n",
    "For the second classification task , we also need to create a training and test split for the classifier. The input for the classifier should contain for each answer the question, the gold answer, the generated answer units by the LLM, and additional features chosen by you that will improve the model's performance. You may use the extracted QA-Pairs from 1a). The dataset should be shuffled with the random seed 42. After shuffling the data, the dataset should be split on question level, meaning 80% of the QA-Pairs should be in the training split and 20% of the QA-Pairs should be in the test split. Prepare the data in such a way that for each data point all the following features: `question`,  `gold answer`, `large language model name`, `llm answer unit`, `trigrams`(create a list of all trigrams for the tokenized an preprocessed answer unit and convert it into one string for adding to the feature dictionary, you can use `ngrams` for creating the list), `word_count`(the number of words in the llm answer unit), `token_overlap`(the number of unique tokens that overlap between the gold answer and the LLM answer unit) are represented as well as one of the six label categories. All of the highlighted features should be included and the features `trigrams`,`word_count` and `token_overlap` should be only used for this task. Preprocess the data for `trigrams`,`word_count` and `token_overlap` in the same way as in 2a) before extracting the features and **exclude** empty answer units. This task will be tested by testing functions and evaluating the classifiers perfomance on the created datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba1b0d9-3931-4606-97b2-809b4b6ab356",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dfbc7c0b3ecf1fe2fba7ff7035b9accf",
     "grade": false,
     "grade_id": "cell-a41a76c7e3e66e1d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_train_test_split_answer_units(qa_pairs: List[Tuple[str, str, str, List[str], str, str, List[str], List[str]]]) -> Tuple[List[Tuple[dict, str]], List[Tuple[dict, str]]]:\n",
    "    '''\n",
    "    Create a feature set and perform a train-test split. The train split is used to train the classifiers on. The test split is used to evaluate the classifiers on.\n",
    "\n",
    "    Args:\n",
    "        qa_pairs (List[tuple]): List of tuples containing information about QA pairs.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[Tuple[dict, str]], List[Tuple[dict, str]]]: \n",
    "        A tuple containing two lists:\n",
    "        - The first list is the train split, where each entry is a tuple with a feature dictionary and its label (6 categories).\n",
    "        - The second list is the test split, with the same format.\n",
    "        - the feature dictionary should look like this: {\"question\": _, \"gold_answer\": _, \"llm_model_name\": _, \"llm_answer_unit\": _, \"trigrams\": _, \"word_count\": _, \"token_overlap\": _}\n",
    "        Example: The features: trigrams, word_count and token_overlap would be: {'trigrams': 'Intrathecal nusinersen shown nusinersen shown effective shown effective treating effective treating infants treating infants spinal infants spinal muscular spinal muscular atrophy muscular atrophy SMA atrophy SMA type SMA type compared type compared sham compared sham procedure',\n",
    "                 'word_count': 14, 'token_overlap': 7} for the following LLM answer unit and gold answer: \n",
    "                 'llm_answer_unit': 'Intrathecal nusinersen has been shown to be effective in treating infants with spinal muscular atrophy (SMA) type I, compared to sham procedure.'\n",
    "                 'gold_answer': 'Reviewers conducted a search in October 2018 and found only a single small RCT assessing nusinersen in infants with SMA type I. Nusinersen may reduce the combined outcome of death or need for fulltime ventilation, and infants may experience fewer severe adverse events compared with a sham procedure; effects on other outcomes were unclear.\n",
    "                                 No firm conclusions can be drawn based on this small study. In addition to the paucity of evidence, nusinersen is incredibly expensive (125,000 USD per dose, with at least four doses in the first year of treatment), is dosed via lumbar/spinal tap, and may require sedation and special treatment centers for administration. Click here for further information.'\n",
    "    '''\n",
    "    train_split, test_split = [],[]\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return train_split, test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc01957-0156-4bb5-997e-74e62f7ff017",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "676079ee65efd96ac45ae77987be8dad",
     "grade": true,
     "grade_id": "cell-4a6aa3b44c93fdf5",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Public Tests\n",
    "train_split_answer_units, test_split_answer_units = create_train_test_split_answer_units(data)\n",
    "print(train_split_answer_units[0])\n",
    "assert isinstance(train_split_answer_units, list)\n",
    "assert isinstance(test_split_answer_units, list)\n",
    "assert isinstance(train_split_answer_units[0][0], dict)\n",
    "assert isinstance(train_split_answer_units[0][1], str)\n",
    "assert isinstance(test_split_answer_units[0][0], dict)\n",
    "assert isinstance(test_split_answer_units[0][1], str)\n",
    "assert all(\"trigrams\" in element[0].keys() for element in train_split_answer_units)\n",
    "assert all(\"trigrams\" in element[0].keys() for element in test_split_answer_units)\n",
    "assert all(\"word_count\" in element[0].keys() for element in train_split_answer_units)\n",
    "assert all(\"word_count\" in element[0].keys() for element in test_split_answer_units)\n",
    "assert all(\"token_overlap\" in element[0].keys() for element in train_split_answer_units)\n",
    "assert all(\"token_overlap\" in element[0].keys() for element in test_split_answer_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aca43a-de04-408d-9033-4f6129d9d7be",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab9123a368486263128fe41d5d9efdb3",
     "grade": true,
     "grade_id": "cell-2b36d5b98caaa2e9",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Private Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd6cdc4-5f12-4740-8511-8158bf09016e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fc91d6178bc90f28761e16bfa00b9e23",
     "grade": false,
     "grade_id": "cell-2ff0f39c8e0227b8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### __c)__ (0 Points)\n",
    "This function is used for training the classifier on the training split created in 2a) and 2b). Feel free to use this function to test the training split created by you. You must use the same vectorizer object for training and testing the neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3708be5a-d01e-49f5-8d77-7701a67886f4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec925844ebc221f82ee56badf1e079af",
     "grade": false,
     "grade_id": "cell-f6cf03cf50a75469",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_cls(train_split: List[Tuple[dict, str]], vectorizer: DictVectorizer) -> Tuple[nltk.DecisionTreeClassifier, MLPClassifier]:\n",
    "    \"\"\"\n",
    "    Train a Decision Tree Classifier (DTC) and a Neural Network Classifier using the provided data.\n",
    "\n",
    "    Parameters:\n",
    "    - train_split (List[Tuple[dict, str]]): A list of tuples containing feature dictionaries and their corresponding labels.\n",
    "    - vectorizer (DictVectorizer): An instance of DictVectorizer for converting feature dictionaries into a vector representation.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[nltk.DecisionTreeClassifier, MLPClassifier]: A tuple containing the trained Decision Tree Classifier (DTC) and Neural Network Classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    # Separate the train set into features and labels\n",
    "    feature_train, labels_train = zip(*train_split)\n",
    "    \n",
    "    # Train the Decision Tree Classifier\n",
    "    dtc = nltk.DecisionTreeClassifier.train(train_split)\n",
    "\n",
    "    # Embed the names into a vector representation\n",
    "    train_vectorized = vectorizer.fit_transform(feature_train)\n",
    "    \n",
    "    # Create the Neural Network Model and train it on the data\n",
    "    neural_network_classifier = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n",
    "    neural_network_classifier.fit(train_vectorized, labels_train)\n",
    "\n",
    "    return dtc, neural_network_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569ac11e-9e2c-43c0-ac98-7cfb490390c4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43ff871aeab4e7b71f17c23d42f35256",
     "grade": true,
     "grade_id": "cell-3bd53ef62051130f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Use this cell to train the classifier on the training split you created.\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a97d1be-e843-45cc-b067-cfc7633a5e47",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a65389131c3b7e3d3ab786c0d1fb5ae",
     "grade": true,
     "grade_id": "cell-97ab7a5ca28a745e",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Private Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0124743e-3245-4e89-94fb-7ed09f67536d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d07586be598454498974df5cf0cbd9b",
     "grade": false,
     "grade_id": "cell-9ae3920d1c7c6545",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### __d)__ (0 Points)\n",
    "This function returns the precision, recall and f1 for a given classifier and label category. Use this function to test the test split created by you.\n",
    "You must use the same vectorizer object for training and testing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f1e706-f41b-466e-836c-eb5236d4382f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0c79815873b9bebebd3ceaef7c65966",
     "grade": false,
     "grade_id": "cell-9d1bb7e41babd641",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_classifier(classifier, vectorizer:DictVectorizer, test_split: List[Tuple[dict, str]], is_nn: bool, label_category: str) -> Tuple[float, float, float]:\n",
    "    '''\n",
    "    Evaluate a classifier using precision, recall, and F1 score for a specific label category.\n",
    "\n",
    "    Args:\n",
    "        classifier (object): The trained classifier model.\n",
    "        vectorizer (DictVectorizer): The vectorizer used to transform features.\n",
    "        test_split (List[Tuple[Dict[str, str], str]]): The test set is a list, where each entry is a tuple with a feature dictionary and its label.\n",
    "        is_nn (bool): Indicates if the given classifier is a neural network (MLPClassifier), otherwise, it is a nltk.DecisionTreeClassifier.\n",
    "        label_category (str): The specific label category for which the evaluation is performed.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float, float]: A tuple containing precision, recall, and F1 score.\n",
    "    '''\n",
    "    precision, recall, f_score = 0, 0, 0\n",
    "    ### Begin Solution\n",
    "    feature_test, labels_test = zip(*test_split)\n",
    "    predictions = []\n",
    "    if is_nn:\n",
    "        # vetorize test features\n",
    "        test_vectorized = vectorizer.transform(feature_test)\n",
    "        predictions =  classifier.predict(test_vectorized)\n",
    "    else: \n",
    "        predictions = [classifier.classify(features) for features in feature_test]\n",
    "    precision = precision_score(labels_test, predictions, labels=[label_category], average='weighted',zero_division=0)\n",
    "    recall= recall_score(labels_test, predictions, labels=[label_category], average='weighted',zero_division=1)\n",
    "    f_score= f1_score(labels_test, predictions, labels=[label_category], average='weighted', zero_division=0)\n",
    "    ### End Solution\n",
    "    return precision, recall, f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bb7aac-693f-461c-a2f2-652917f66187",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b5f725d6420456f191117b8d66de9cf",
     "grade": true,
     "grade_id": "cell-70d7775e78545e00",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Use this cell to evaluate the classifier on the test split you created. \n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ece9b05-4394-47f2-acd7-d98e9581feeb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb69869582027a93ed20670988715e34",
     "grade": true,
     "grade_id": "cell-4b239c3a6d15435f",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS TEST CELL\n",
    "# Private Tests"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
