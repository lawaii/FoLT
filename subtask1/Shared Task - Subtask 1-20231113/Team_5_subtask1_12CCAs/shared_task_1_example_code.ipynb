{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0504f6d-a9fe-434e-82c8-72e80eb18ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486abfea-44ce-4905-b5d6-f8f1d356dc30",
   "metadata": {},
   "source": [
    "### Step A: open file and check if the code get the right .json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e654019-ab12-4ce6-8409-6c6be53a54cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team_5_subtask1_12CCAs_2277051.json\n",
      "Team_5_subtask1_12CCAs_2314325.json\n",
      "Team_5_subtask1_12CCAs_2401157.json\n",
      "team size:3\n"
     ]
    }
   ],
   "source": [
    "# open file and check if the code get the right .json files\n",
    "files = []\n",
    "for f_name in glob('*.json'):\n",
    "    print(f_name)\n",
    "    with open(f_name, encoding='utf-8') as f:\n",
    "        file = json.load(f)\n",
    "        files.append([f_name, file])\n",
    "print('team size:' + str(len(files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcad3a8-1e64-47f4-8377-b87adb783202",
   "metadata": {},
   "source": [
    "### Step B: Check if all the .json files are in the right format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a6fcb09d-1c54-4f10-a0f3-d6b01a00355a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test passed\n"
     ]
    }
   ],
   "source": [
    "error = 0\n",
    "for [f_name, file] in files:\n",
    "    try:\n",
    "        assert len(file) == 24,  f\"{f_name} must have 24 annotations\"\n",
    "    except AssertionError as e:\n",
    "        print(f\"{e}\")\n",
    "        error = error + 1\n",
    "\n",
    "    \n",
    "    for i, annotation in enumerate(file):\n",
    "        try:\n",
    "            assert \"result\" in annotation[\"annotations\"][0][\"prediction\"], f\"{f_name} must have pre-annotated prediction\"\n",
    "            \n",
    "        except AssertionError as e:\n",
    "            print(f\"{e}\")\n",
    "            error = error + 1\n",
    "            \n",
    "        pre_annotated_id = [result[\"id\"] for result in annotation[\"annotations\"][0][\"prediction\"][\"result\"]]\n",
    "        annotated_id = [result[\"id\"] for result in annotation[\"annotations\"][0][\"result\"]]\n",
    "        try:\n",
    "            # print(\" \")\n",
    "            # print(annotation[\"annotations\"][0][\"result\"][-1][\"from_name\"])\n",
    "            # print(\"question_type\")\n",
    "            # print(\" \")\n",
    "            assert annotation[\"annotations\"][0][\"result\"][-1][\"from_name\"] == \"question_type\", f\"{f_name}: question_type must be annotated and should be the first one\"\n",
    "        except AssertionError as e:\n",
    "            print(f\"{e}\")\n",
    "            error = error + 1\n",
    "\n",
    "       \n",
    "            \n",
    "        for annotated_idx in annotated_id[:-1]:\n",
    "            if annotated_idx in pre_annotated_id:\n",
    "                continue\n",
    "            else:\n",
    "                try:\n",
    "                    # print(\" \")\n",
    "                    # print(annotated_idx)\n",
    "                    # print(pre_annotated_id)\n",
    "                    # print(\" \")\n",
    "                    assert False, f\"{f_name}: annotation_idx is not in pre_annotated_id pre have different id\"\n",
    "                except AssertionError as e:\n",
    "                    print(f\"{e}\")\n",
    "                    error = error + 1\n",
    "\n",
    "        \n",
    "\n",
    "if error == 0:\n",
    "    print(\"test passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da33ec02-cbfb-4f90-8ff0-bdba96336009",
   "metadata": {},
   "source": [
    "### Step C: Calculate cohen kappa score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c0f37ea-ca9a-4aae-a543-543604a289ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save annotation results into a dict\n",
    "def get_annotations(l:list):\n",
    "    \"\"\"\n",
    "    save annotation results of a json file into annotation_dict\n",
    "    \n",
    "    Parameters\n",
    "    ------\n",
    "    l: list\n",
    "        items in files\n",
    "    \n",
    "    return: annotation_dict: dict\n",
    "                annotation result\n",
    "            annotation_dict[\"Student_ID\"]: string\n",
    "                Student_ID in f_name\n",
    "            annotation_dict[\"IDs\"]: list\n",
    "                list of annotation IDs of every task\n",
    "            annotation_dict[\"question_type\"]: list\n",
    "                list of question_type of every task\n",
    "            annotation_dict[\"harmfulness\"]: list\n",
    "                list of harmfulness of every task\n",
    "            annotation_dict[\"sent_cate_dict\"]: dict\n",
    "                key: string\n",
    "                    annotation IDs\n",
    "                value: list\n",
    "                    sents_category of every units\n",
    "    \"\"\"\n",
    "    annotation_dict = {\"Student_ID\": l[0].split(\"_\")[4].replace(\".json\",\"\")}\n",
    "    IDs = []\n",
    "    question_type = []\n",
    "    harmfulness = []\n",
    "    sent_cate_dict = {}\n",
    "    for data in l[1]:\n",
    "        results = data['annotations'][0]['result']\n",
    "        harm = 'non-harmful'\n",
    "        sents_categories = []\n",
    "        for result in results:\n",
    "            if result['from_name'] == 'llm_answer_label_1':\n",
    "                validity = result['value']['choices'][0]\n",
    "                ID = result['id']\n",
    "                IDs.append(ID)\n",
    "            elif result['from_name'] == 'question_type':\n",
    "                question_type.append(result['value']['choices'][0])\n",
    "            elif result['from_name'] == 'llm_answer_fine_grain_label':\n",
    "                sents_categories.append(result['value']['labels'][0])\n",
    "                if result['value']['labels'][0] == \"Contradiction\" or result['value']['labels'][0] == \"Exaggeration\":\n",
    "                    harm = 'harmful'\n",
    "        # if answer is Invalid, then all units are invalid \n",
    "        if validity == \"Invalid\":\n",
    "            for i in range(len(sents_categories)):\n",
    "                sents_categories[i] = \"Invalid\"\n",
    "        sent_cate_dict[ID] = sents_categories\n",
    "        harmfulness.append(harm)\n",
    "    annotation_dict[\"question_type\"] = question_type\n",
    "    annotation_dict[\"harmfulness\"] = harmfulness\n",
    "    annotation_dict[\"sent_cate_dict\"] = sent_cate_dict\n",
    "    return annotation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa60a2d3-95a2-491e-bd4e-a6d46ac7b8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit annotations categories, len(categories) = 6\n",
    "categories = [\"Contradiction\", \"Exaggeration\", \"Understatement\", \"Agree with the gold answer\", \"Cannot assess\", \"General comment\"]\n",
    "# get answer_fine_grained_per_category\n",
    "def fine_grain(l:list):\n",
    "    \"\"\"\n",
    "    put answer_fine_grained_per_category into annotation_dict\n",
    "    \n",
    "    Parameters\n",
    "    ------\n",
    "    l: list\n",
    "        items in files\n",
    "    \n",
    "    return: annotation_dict: add fine_grained_unit_annotation overall and per category as lists\n",
    "    \"\"\"\n",
    "    annotation_dict = get_annotations(l)\n",
    "    # list of all the unit annotations\n",
    "    overall_sents_categories_list = [c for sents_cate in annotation_dict[\"sent_cate_dict\"].values() for c in sents_cate]\n",
    "    annotation_dict[\"overall_sents_categories_list\"] = overall_sents_categories_list\n",
    "    # change all the unit annotations into either label or \"Others\", \n",
    "    for label in categories:\n",
    "        annotation_dict[label] = [\"Others\" if i != label else label for i in overall_sents_categories_list]\n",
    "    return annotation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "77fe6fc3-c71e-4a49-b6f1-8d0a3483793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohen_kappa(annotation_1, annotation_2, labels):\n",
    "    po = len([[label1, label2] for label1, label2 in zip(annotation_1, annotation_2) if label1 == label2]) / len(annotation_1)\n",
    "    \n",
    "    pe = 0\n",
    "    for l in labels:\n",
    "        p1 = len([label for label in annotation_1 if label == l ]) / len(annotation_1)\n",
    "        p2 = len([label for label in annotation_2 if label == l ]) / len(annotation_2)\n",
    "        pe = pe + p1 * p2\n",
    "        \n",
    "    if pe == 1:\n",
    "        k = 1\n",
    "    else:\n",
    "        k = (po - pe) / (1 - pe)\n",
    "#    print(po, p1, p2, pe)\n",
    "    return k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8f5ab526-f816-4948-9167-c7d3b14c9aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cohen kappa score for 2 students\n",
    "question_type_labels = ['1. Yes/no question', '2. Open ended - Comparison of different specific interventions', '3. Open ended - Specific effect of a specific intervention','4. Open ended - General effects of a specific intervention',\n",
    "  '5. Open ended - Comparison of different nonspecific interventions']\n",
    "validity_labels =['Valid', 'Invalid']\n",
    "harmfulness_labels = ['non-harmful', 'harmful']\n",
    "\n",
    "# expand unit annotations categories with \"\"Invalid\"\" as labels for \"Overall Cohen’s kappa on sentence categories\", len(categories) = 7\n",
    "categories.append(\"Invalid\")\n",
    "\n",
    "def calculate_cohen_kappa(student_1_result, student_2_result):\n",
    "    x = files[0][0].split(\"_\")[1]\n",
    "    data = {\n",
    "        f\"Team_{x}_student1ID\": student_1_result[\"Student_ID\"],\n",
    "        f\"Team_{x}_student2ID\": student_2_result[\"Student_ID\"], \n",
    "        \"Overall Cohen’s kappa on question type\": cohen_kappa(student_1_result[\"question_type\"], student_2_result[\"question_type\"], question_type_labels), \n",
    "        \"Overall Cohen’s kappa on sentence categories\": cohen_kappa(student_1_result[\"overall_sents_categories_list\"], student_2_result[\"overall_sents_categories_list\"], categories), \n",
    "        \"Cohen’s kappa on contradiction\": cohen_kappa(student_1_result[\"Contradiction\"], student_2_result[\"Contradiction\"], [\"Contradiction\", 'Others']), \n",
    "        \"Cohen’s kappa on exaggeration\": cohen_kappa(student_1_result[\"Exaggeration\"], student_2_result[\"Exaggeration\"], [\"Exaggeration\", 'Others']), \n",
    "        \"Cohen’s kappa on understatement\": cohen_kappa(student_1_result[\"Understatement\"], student_2_result[\"Understatement\"], [\"Understatement\", 'Others']), \n",
    "        \"Cohen’s kappa on agree with the gold answer\": cohen_kappa(student_1_result[\"Agree with the gold answer\"], student_2_result[\"Agree with the gold answer\"], [\"Agree with the gold answer\", 'Others']), \n",
    "        \"Cohen’s kappa on cannot assess\": cohen_kappa(student_1_result[\"Cannot assess\"], student_2_result[\"Cannot assess\"], [\"Cannot assess\", 'Others']), \n",
    "        \"Cohen’s kappa on general comment\": cohen_kappa(student_1_result[\"General comment\"], student_2_result[\"General comment\"], [\"General comment\", 'Others']), \n",
    "        \"Cohen’s kappa on harmfulness\": cohen_kappa(student_1_result[\"harmfulness\"], student_2_result[\"harmfulness\"], harmfulness_labels)\n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f71d8f6f-083e-41ab-833c-554a863dc991",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_1_result = fine_grain(files[0])\n",
    "student_2_result = fine_grain(files[1])\n",
    "\n",
    "df = pd.DataFrame(calculate_cohen_kappa(student_1_result, student_2_result), index=[0])\n",
    "\n",
    "# if your team have 3 members\n",
    "if len(files) == 3:\n",
    "    student_3_result = fine_grain(files[2])\n",
    "    for student_result in [student_1_result, student_2_result]:\n",
    "        new_row = calculate_cohen_kappa(student_result, student_3_result)\n",
    "        # df = df.append(new_row, ignore_index=True)\n",
    "        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# generate .csv file name\n",
    "x = files[0][0].split(\"_\")[1]\n",
    "#print(f\"Team_{x}_subtask1_12CCAs_Kappa score.csv\")\n",
    "df.to_csv(f\"Team_{x}_subtask1_12CCAs_Kappa score.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d132d1c7-940b-41da-98e4-952ad6bc1b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team_5_student1ID</th>\n",
       "      <th>Team_5_student2ID</th>\n",
       "      <th>Overall Cohen’s kappa on question type</th>\n",
       "      <th>Overall Cohen’s kappa on sentence categories</th>\n",
       "      <th>Cohen’s kappa on contradiction</th>\n",
       "      <th>Cohen’s kappa on exaggeration</th>\n",
       "      <th>Cohen’s kappa on understatement</th>\n",
       "      <th>Cohen’s kappa on agree with the gold answer</th>\n",
       "      <th>Cohen’s kappa on cannot assess</th>\n",
       "      <th>Cohen’s kappa on general comment</th>\n",
       "      <th>Cohen’s kappa on harmfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2277051</td>\n",
       "      <td>2314325</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.383182</td>\n",
       "      <td>0.305808</td>\n",
       "      <td>-0.049216</td>\n",
       "      <td>-0.006920</td>\n",
       "      <td>0.322984</td>\n",
       "      <td>0.362471</td>\n",
       "      <td>0.647747</td>\n",
       "      <td>0.117647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2277051</td>\n",
       "      <td>2401157</td>\n",
       "      <td>0.213115</td>\n",
       "      <td>0.294047</td>\n",
       "      <td>0.322731</td>\n",
       "      <td>0.117565</td>\n",
       "      <td>-0.006920</td>\n",
       "      <td>0.165931</td>\n",
       "      <td>0.298718</td>\n",
       "      <td>0.451602</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2314325</td>\n",
       "      <td>2401157</td>\n",
       "      <td>0.213115</td>\n",
       "      <td>0.192486</td>\n",
       "      <td>0.112025</td>\n",
       "      <td>-0.016298</td>\n",
       "      <td>-0.010417</td>\n",
       "      <td>0.268736</td>\n",
       "      <td>0.094540</td>\n",
       "      <td>0.407110</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Team_5_student1ID Team_5_student2ID  Overall Cohen’s kappa on question type  \\\n",
       "0           2277051           2314325                                0.769231   \n",
       "1           2277051           2401157                                0.213115   \n",
       "2           2314325           2401157                                0.213115   \n",
       "\n",
       "   Overall Cohen’s kappa on sentence categories  \\\n",
       "0                                      0.383182   \n",
       "1                                      0.294047   \n",
       "2                                      0.192486   \n",
       "\n",
       "   Cohen’s kappa on contradiction  Cohen’s kappa on exaggeration  \\\n",
       "0                        0.305808                      -0.049216   \n",
       "1                        0.322731                       0.117565   \n",
       "2                        0.112025                      -0.016298   \n",
       "\n",
       "   Cohen’s kappa on understatement  \\\n",
       "0                        -0.006920   \n",
       "1                        -0.006920   \n",
       "2                        -0.010417   \n",
       "\n",
       "   Cohen’s kappa on agree with the gold answer  \\\n",
       "0                                     0.322984   \n",
       "1                                     0.165931   \n",
       "2                                     0.268736   \n",
       "\n",
       "   Cohen’s kappa on cannot assess  Cohen’s kappa on general comment  \\\n",
       "0                        0.362471                          0.647747   \n",
       "1                        0.298718                          0.451602   \n",
       "2                        0.094540                          0.407110   \n",
       "\n",
       "   Cohen’s kappa on harmfulness  \n",
       "0                      0.117647  \n",
       "1                      0.200000  \n",
       "2                      0.272727  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ce0b03-58e2-4e1d-aa1f-d9dba4c5a945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36d99eb-52b7-43a9-a2d6-ea3b8e1e0c67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
